akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 30s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  actor {
    provider = akka.cluster.ClusterActorRefProvider

    #this dispatcher uses up to 6 threads
    default-dispatcher {
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 4
      }
    }

    allow-java-serialization = off

    serializers {
      jrSerializer = "com.safechat.serializer.JournalEventsSerializer"
    }

    serialization-bindings {
      "com.safechat.domain.MsgEnvelope" = jrSerializer
      "com.safechat.actors.FullChatState" = jrSerializer
    }
  }

  http {
    server {
      max-connections = 1024
      idle-timeout = infinite
      socket-options {
        // Set to demonstrate smaller OS buffers
        # so-receive-buffer-size = 10000
        # so-send-buffer-size = 10000
        # so-receive-buffer-size = 1024
        # so-send-buffer-size = 1024
      }
    }
  }

  #remote {
  #  log-remote-lifecycle-events = off
  #  netty.tcp {
  #    hostname = 192.168.77.10
  #    port = 2551
  #  }
  #}

  #remote = {
  #  artery {
  #    enabled = true
  #    transport = tcp
  #    canonical.port = 2551
  #    canonical.hostname = 192.168.77.10
  #  }
  #}

  cluster {

    # General recomentation how to pick AutoDowning provides is following:
    # If you have a static cluster (like always 3 or 5 nodes) - use Static Quorum (QuorumLeaderAutoDowning)
    # If you have a mode flexible scenarion where you scale up for 5 tp 9 and down to 7 - use Keep Majority (MajorityLeaderAutoDowning)

    #https://github.com/TanUkkii007/akka-cluster-custom-downing#akka-cluster-custom-downing

    # MajorityLeaderAutoDowning is similar to  QuorumLeaderAutoDowning. However, instead of a static specified quorum size
    # this strategy automatically keeps the partition with the largest amount of nodes. If the partitions are of equal size,
    # the partition that contains the node with the globally lowest address is kept. The strategy is the same as the keep majority
    # strategy of Split Brain Resolver from Typesafe reactive platform. If a role is set by majority-member-role,
    # the strategy is only enforced to the nodes with the specified role.
    downing-provider-class = "tanukki.akka.cluster.autodown.MajorityLeaderAutoDowning"

    custom-downing {

      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 5s

      majority-leader-auto-downing {
        majority-member-role = ""
        down-if-in-minority = true
        shutdown-actor-system-on-resolution = true
      }
    }

    # CoordinatedShutdown will run the tasks that are added to these
    # phases. The phases can be ordered as a DAG by defining the
    # dependencies between the phases.
    # Each phase is defined as a named config section with the
    # following optional properties:
    # - timeout=15s: Override the default-phase-timeout for this phase.
    # - recover=off: If the phase fails the shutdown is aborted
    #                and depending phases will not be executed.
    # depends-on=[]: Run the phase after the given phases
    coordinated-shutdown {
      # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
      # if this is set to 'on'. It is done after termination of the
      # ActorSystem if terminate-actor-system=on, otherwise it is done
      # immediately when the last phase is reached.
      exit-jvm = on
      default-phase-timeout = 10 seconds
    }

    roles = ["chat"]

    metrics.enabled = off

    #seed-nodes = [ "akka://echatter@192.168.77.10:2551" ]

    sharding {
      buffer-size = 1000

      role = "chat"

      #Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # How often the coordinator saves persistent snapshots, which are
      # used to reduce recovery times
      snapshot-interval = 120 s

      journal-plugin-id = "cassandra-journal"
      snapshot-plugin-id = "cassandra-snapshot-store"

      # Rebalance check is performed periodically with this interval
      rebalance-interval = 30 s

      snapshot-after = 7200
      waiting-for-state-timeout = 5 s
      updating-state-timeout = 5 s
      use-dispatcher = shard-dispatcher

      remember-entities = on # default: off

      #state-store-mode = ddata
    }
  }

  persistence {
    journal {
      max-message-batch-size = 200
      max-confirmation-batch-size = 10000
      max-deletion-batch-size = 10000
      plugin = "cassandra-journal"
    }
    snapshot-store {
      plugin = "cassandra-snapshot-store"
    }
  }

}

cassandra-journal {

  #cluster-id = "_cluster"

  #local-datacenter = "west"

  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # Comma-separated list of contact points in the cluster
  #"192.168.77.5", "192.168.5.116", "130.193.44.21"
  #contact-points = ["95.142.47.77","95.142.47.136"]

  authentication.username = "fsa"
  authentication.password = "ncXMbELjuDycnokVhnQowLaFzcsPfnRJrmgTAeRmxouuexrcQFdx3mBPzcJNawEy"

  # Port of contact points in the cluster
  port = 9042

  # Name of the keyspace to be created/used by the journal
  keyspace = "chat"

  # Name of the table to be created/used by the journal
  table = "chat_journal"

  # Replication factor to use when creating a keyspace
  replication-factor = 3

  #data-center-replication-factors = ["west:2", "east:2"]

  # Write consistency level
  write-consistency = "QUORUM"
  #"ANY"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of entries per partition (= columns per row).
  # Must not be changed after table creation (currently not checked).
  target-partition-size = 4096
  #500000

  # Maximum size of result set
  max-result-size = 4096

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "shard-dispatcher"

  # Dispatcher for fetching and replaying messages
  replay-dispatcher = "shard-dispatcher"

  # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
  # false to avoid the overhead of maintaining the tag_views table.
  events-by-tag.enabled = false
}

cassandra-snapshot-store {

  #cluster-id = "_cluster"

  #local-datacenter = "west"

  # FQCN of the cassandra snapshot store plugin
  class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

  # Parameter indicating whether the journal keyspace should be auto created
  keyspace-autocreate = true

  # Parameter indicating whether the journal tables should be auto created
  tables-autocreate = true

  # Comma-separated list of contact points in the cluster
  # contact-points = ["192.168.77.5", "192.168.5.116"]

  authentication.username = "fsa"
  authentication.password = "ncXMbELjuDycnokVhnQowLaFzcsPfnRJrmgTAeRmxouuexrcQFdx3mBPzcJNawEy"

  # Port of contact points in the cluster
  port = 9042

  # Name of the keyspace to be created/used by the snapshot store
  keyspace = "chat"

  # Name of the table to be created/used by the snapshot store
  table = "chat_snapshot"

  # Replication factor to use when creating a keyspace
  replication-factor = 3

  #data-center-replication-factors = ["west:2", "east:2"]

  # Write consistency level
  write-consistency = "QUORUM"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of snapshot metadata to load per recursion (when trying to
  # find a snapshot that matches specified selection criteria). Only increase
  # this value when selection criteria frequently select snapshots that are
  # much older than the most recent snapshot i.e. if there are much more than
  # 10 snapshots between the most recent one and selected one. This setting is
  # only for increasing load efficiency of snapshots.
  max-metadata-result-size = 10

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "shard-dispatcher"
}

shard-dispatcher {
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 2
    parallelism-max = 4
  }
}

http-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 2
    parallelism-max = 8
  }
}


fixed-thread-pool {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 5
  }
  throughput = 2
}

resizable-thread-pool {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    core-pool-size-min = 5
    core-pool-size-factor = 2.0
    core-pool-size-max = 20
  }
  throughput = 1000
}