akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 30s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  actor {

    provider = akka.cluster.ClusterActorRefProvider

    #this dispatcher uses up to 6 threads
    default-dispatcher {
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 4
      }
    }

    allow-java-serialization = on #off

    serializers {
      journalSerializer = "akka.io.JournalEventsSerializer2"
    }

    serialization-bindings {
      //events
      "com.safechat.actors.UserJoined"        = journalSerializer
      "com.safechat.actors.UserTextAdded"     = journalSerializer
      "com.safechat.actors.UserDisconnected"  = journalSerializer

      //state
      "com.safechat.actors.ChatRoomState"     = journalSerializer
    }
  }

  http {
    server {
      max-connections = 1024
      idle-timeout = infinite
      socket-options {
        // Set to demonstrate smaller OS buffers
        # so-receive-buffer-size = 10000
        # so-send-buffer-size = 10000
        # so-receive-buffer-size = 1024
        # so-send-buffer-size = 1024
      }
    }
  }

  cluster {

    #failure-detector {
    #  implementation-class = "akka.remote.PhiAccrualFailureDetector"
    #  threshold = 10 # 8
    #  heartbeat-interval = 1 s
    #  acceptable-heartbeat-pause = 4 s #3
    #}

    # General recomentation how to pick AutoDowning provides is following:
    # If you have a static cluster (like always 3 or 5 nodes) - use static-quorum
    # If you have a more flexible scenarion where you scale up for 5 t0 9 and down to 7 - use keep-majority

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    #
    split-brain-resolver {

      # static-quorum, keep-majority, keep-oldest, down-all, lease-majority
      active-strategy = keep-majority

      #//#stable-after
      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 7s

      # When reachability observations by the failure detector are changed the SBR decisions
      # are deferred until there are no changes within the 'stable-after' duration.
      # If this continues for too long it might be an indication of an unstable system/network
      # and it could result in delayed or conflicting decisions on separate sides of a network
      # partition.
      # As a precaution for that scenario all nodes are downed if no decision is made within
      # `stable-after + down-all-when-unstable` from the first unreachability event.
      # The measurement is reset if all unreachable have been healed, downed or removed, or
      # if there are no changes within `stable-after * 2`.
      # The value can be on, off, or a duration.
      # By default it is 'on' and then it is derived to be 3/4 of stable-after.
      down-all-when-unstable = on

    }


    # CoordinatedShutdown will run the tasks that are added to these
    # phases. The phases can be ordered as a DAG by defining the
    # dependencies between the phases.
    # Each phase is defined as a named config section with the
    # following optional properties:
    # - timeout=15s: Override the default-phase-timeout for this phase.
    # - recover=off: If the phase fails the shutdown is aborted
    #                and depending phases will not be executed.
    # depends-on=[]: Run the phase after the given phases
    coordinated-shutdown {
      # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
      # if this is set to 'on'. It is done after termination of the
      # ActorSystem if terminate-actor-system=on, otherwise it is done
      # immediately when the last phase is reached.
      exit-jvm = on #needed for k83

      default-phase-timeout = 10 seconds
    }

    metrics.enabled = off

    sharding {

      buffer-size = 1000

      #Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # How often the coordinator saves persistent snapshots, which are
      # used to reduce recovery times
      snapshot-interval = 120 s

      journal-plugin-id = "cassandra-journal"
      snapshot-plugin-id = "cassandra-snapshot-store"

      # Rebalance check is performed periodically with this interval
      rebalance-interval = 30 s

      snapshot-after = 7200
      waiting-for-state-timeout = 5 s
      updating-state-timeout = 5 s
      use-dispatcher = shard-dispatcher

      #remember-entities = on # default: off
      #state-store-mode = ddata
    }
  }


  persistence {

      #https://doc.akka.io/docs/akka/current/typed/persistence.html#replay-filter
      #https://blog.softwaremill.com/akka-cluster-split-brain-failures-are-you-ready-for-it-d9406b97e099
      #So if a split brain produces more wrong events than window-size then your aggregate state will be corrupted
      journal-plugin-fallback.replay-filter.window-size = 150 #100

      journal {
        auto-start-journals = ["akka.persistence.cassandra.journal"]
        plugin = "akka.persistence.cassandra.journal"
      }

      snapshot-store {
        auto-start-snapshot-stores = ["akka.persistence.cassandra.snapshot"]
        plugin = "akka.persistence.cassandra.snapshot"
      }

      cassandra {

        journal {
          keyspace-autocreate = true
          tables-autocreate = true

          keyspace = "chat"
          table = "chat_journal"

          # Maximum number of messages that will be batched when using `persistAsync`.
          # Also used as the max batch size for deletes.
          max-message-batch-size = 200

          # Target number of entries per partition (= columns per row).
          # Must not be changed after table creation (currently not checked).
          # This is "target" as AtomicWrites that span partition boundaries will result in bigger partitions to ensure atomicity.
          target-partition-size = 500000

          replication-factor = 3
        }

        events-by-tag {
          # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
          # false to avoid the overhead of maintaining the tag_views table.
          enabled = false

          #first-time-bucket = "20200101T00:00"
        }

        snapshot {
          keyspace-autocreate = true
          tables-autocreate = true

          keyspace = "chat_snapshot"
          table = "chat_snapshots_journal"

          replication-factor = 3
        }
      }
  }

  stream.materializer {
    initial-input-buffer-size = 16
    max-input-buffer-size = 16
    max-fixed-buffer-size = 16
    dispatcher = http-dispatcher

    stream-ref {
      buffer-capacity = 16
      subscription-timeout = 3 seconds
    }
  }
}

shard-dispatcher {
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 2
    parallelism-max = 4
  }
}

http-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 2
    parallelism-max = 8
  }
}

fixed-thread-pool {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 5
  }
  throughput = 2
}

resizable-thread-pool {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    core-pool-size-min = 4
    core-pool-size-factor = 2.0
    core-pool-size-max = 8
  }
  throughput = 1000
}

datastax-java-driver {

  advanced {

    reconnect-on-init = true

    auth-provider {
      class = PlainTextAuthProvider
      username = ...
      password = ...
    }
  }

  basic {
    #contact-points = [""]
    load-balancing-policy.local-datacenter = "datacenter1"
  }

}

safe-chat {

}