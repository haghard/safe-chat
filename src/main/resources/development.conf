akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 30s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = on
  log-dead-letters-during-shutdown = false
  stream.materializer.debug-logging = on

  actor {

    provider = akka.cluster.ClusterActorRefProvider

    default-dispatcher {
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 4
      }
    }

    allow-java-serialization = on #off

    serializers {

      journalSerializer = "akka.io.JournalEventsSerializer"

      commandsSerializer = "akka.io.CommandsSerializer"

      #akka-stream-ref = "akka.stream.serialization.StreamRefSerializer"
    }

    serialization-bindings {

      //events
      "com.safechat.actors.ChatRoomEvent$UserJoined" = journalSerializer
      "com.safechat.actors.ChatRoomEvent$UserTextAdded" = journalSerializer
      "com.safechat.actors.ChatRoomEvent$UserDisconnected" = journalSerializer


      //state
      "com.safechat.actors.ChatRoomState" = journalSerializer

      //commands
      "com.safechat.actors.Command$JoinUser" = commandsSerializer
      "com.safechat.actors.Command$PostText" = commandsSerializer
      "com.safechat.actors.Command$Leave" = commandsSerializer
      "com.safechat.actors.Command$HandOffChatRoom" = commandsSerializer

      //TODO: replies ???
    }

  }

  http {
    server {
      max-connections = 1024

      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle connection timeouts.
      idle-timeout = infinite

      socket-options {
        // Set to demonstrate smaller OS buffers
        # so-receive-buffer-size = 10000
        # so-send-buffer-size = 10000
        # so-receive-buffer-size = 1024
        # so-send-buffer-size = 1024
      }
    }
  }

  remote {
    artery {
      # To notice large messages you can enable logging of message types with payload size in bytes larger than the configured
      log-frame-size-exceeding = 30 KiB #10000 b

      advanced {

        maximum-frame-size = 32 KiB

        # Maximum serialized message size for the large messages, including header data.
        # It is currently restricted to 1/8th the size of a term buffer that can be
        # configured by setting the 'aeron.term.buffer.length' system property.
        # See 'large-message-destinations'.
        maximum-large-frame-size = 2 MiB

        #https://discuss.lightbend.com/t/how-to-avoid-nodes-to-be-quarantined-in-akka-cluster/1932/2

        #That should be rare, but for example if many actors stop at the same time and there are watchers of these actors on other nodes there
        # may be a storm of Terminated messages sent more quickly than they can be delivered and thereby filling up buffers.

        # This setting defines the maximum number of unacknowledged system messages
        # allowed for a remote system. If this limit is reached the remote system is
        # declared to be dead and its UID marked as quarantined.
        system-message-buffer-size = 20000  #The buffer is an ArrayDeque so it grows as needed, but doesn’t shrink.
        outbound-control-queue-size = 20000
      }

      large-message-destinations = [
        # This setting disables log-frame-size-exceeding and maximum-frame-size so you won't see
        # akka.remote.OversizedPayloadException: Discarding oversized payload sent to Some(Actor[akka://dsim@127.0.0.1:2552/user/worker/consumer-controller#-941039105]): max allowed size 32768 bytes. Message type [akka.actor.typed.delivery.ConsumerController$SequencedMessage].

        #"/user/worker/consumer-controller"
      ]
    }
  }


  # CoordinatedShutdown will run the tasks that are added to these
  # phases. The phases can be ordered as a DAG by defining the
  # dependencies between the phases.
  # Each phase is defined as a named config section with the
  # following optional properties:
  # - timeout=15s: Override the default-phase-timeout for this phase.
  # - recover=off: If the phase fails the shutdown is aborted
  #                and depending phases will not be executed.
  # depends-on=[]: Run the phase after the given phases
  coordinated-shutdown {
    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on #needed for k83

    default-phase-timeout = 10 seconds

    phases {

      # Wait until exiting has been completed
      cluster-exiting-done {
        timeout = 12 s # increase if to many leases|chatrooms need to be released.
        depends-on = [cluster-exiting]
      }
    }
  }

  cluster {


    //defalfs
    failure-detector {
      implementation-class = "akka.remote.PhiAccrualFailureDetector"
      threshold = 8
      heartbeat-interval = 1 s
      acceptable-heartbeat-pause = 3 s
    }

    # General recomentation how to pick active-strategy is the following:
    # If you have a static cluster (like always 3 or 5 nodes) - use static-quorum
    # If you have a more flexible scenarion where you scale up for 5 to 9 and down to 7 - use keep-majority

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    #
    split-brain-resolver {

      # static-quorum, keep-majority, keep-oldest, down-all, lease-majority
      # active-strategy = keep-majority

      # Keep the part that can acquire the lease, and down the other part.
      # Best effort is to keep the side that has most nodes, i.e. the majority side.
      # This is achieved by adding a delay before trying to acquire the lease on the
      # minority side.
      active-strategy = lease-majority
      lease-majority {
        lease-implementation = "akka.coordination.lease.cassandra"

        # This delay is used on the minority side before trying to acquire the lease,
        # as an best effort to try to keep the majority side.
        acquire-lease-delay-for-minority = 2s
      }


      #keep-majority tracks the size of the cluster and uses a majority to determine the action to take.
      # If nodes are in communication with a majority of nodes, they will down all unreachable nodes.
      # If a majority can not be reached, they will terminate themselves.
      # The majority is re-calculated whenever nodes are added or removed from the cluster.

      #//#stable-after
      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 5s

      # When reachability observations by the failure detector are changed the SBR decisions
      # are deferred until there are no changes within the 'stable-after' duration.
      # If this continues for too long it might be an indication of an unstable system/network
      # and it could result in delayed or conflicting decisions on separate sides of a network
      # partition.
      # As a precaution for that scenario all nodes are downed if no decision is made within
      # `stable-after + down-all-when-unstable` from the first unreachability event.
      # The measurement is reset if all unreachable have been healed, downed or removed, or
      # if there are no changes within `stable-after * 2`.
      # The value can be on, off, or a duration.
      # By default it is 'on' and then it is derived to be 3/4 of stable-after.
      down-all-when-unstable = on

    }

    metrics.enabled = off

    sharding {
      use-dispatcher = cassandra-dispatcher

      # A lease is a final backup that means that each shard won’t create child entity actors unless it has the lease.

      # Config path of the lease that each shard must acquire before starting entity actors
      # default is no lease
      # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties
      #use-lease = "akka.coordination.lease.cassandra"


      # The interval between retries for acquiring the lease
      lease-retry-interval = 10 s #stable-after = 5s + X

      # Provide a higher level of details in the debug logs, often per routed message. Be careful about enabling in production systems.
      verbose-debug-logging = on


      # Maximum number of messages that are buffered by a ShardRegion actor.
      buffer-size = 100000

      #Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # How often the coordinator saves persistent snapshots, which are
      # used to reduce recovery times
      snapshot-interval = 120 s


      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      # rememberEntities = off ensures that a shard entity won't be recreates/restarted automatically on
      # a different `ShardRegion` due to rebalance, crash or leave (graceful exit). That is exactly what we want,
      # because we want lazy start for ChatRooms.
      remember-entities = off

      # Defines how the coordinator stores its state.
      # Same is also used by the shards for rememberEntities.
      # Valid values are "ddata" or "persistence". "persistence" mode is deprecated
      state-store-mode = persistence

      # Set this to a time duration to have sharding passivate entities when they have not
      # received any message in this length of time. Set to 'off' to disable.
      # It is always disabled if `remember-entities` is enabled.

      passivate-idle-entity-after = 120s


      # Rebalance check is performed periodically with this interval
      rebalance-interval = 30 s

      #snapshot-after = 1000

      #waiting-for-state-timeout = 5 s
      #updating-state-timeout = 5 s

      use-dispatcher = cassandra-dispatcher

    }
  }


  # CoordinatedShutdown will run the tasks that are added to these
  # phases. The phases can be ordered as a DAG by defining the
  # dependencies between the phases.
  # Each phase is defined as a named config section with the
  # following optional properties:
  # - timeout=15s: Override the default-phase-timeout for this phase.
  # - recover=off: If the phase fails the shutdown is aborted
  #                and depending phases will not be executed.
  # depends-on=[]: Run the phase after the given phases
  coordinated-shutdown {
    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on
    default-phase-timeout = 10 seconds
  }

  extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]

  discovery {
    # method = akka-dns
  }

  //https://doc.akka.io/docs/akka-management/current/akka-management.html
  management {


    http {

      base-path = "fsa"

      #port = 8558
      #bind-port = 8558

      #hostname
      #bind-hostname
      route-providers-read-only = false
      
    }

    cluster.bootstrap {

      # Cluster Bootstrap will always attempt to join an existing cluster if possible. However
      # if no contact point advertises any seed-nodes a new cluster will be formed by the
      # node with the lowest address as decided by [[LowestAddressJoinDecider]].
      # Setting `new-cluster-enabled=off` after an initial cluster has formed is recommended to prevent new clusters
      # forming during a network partition when nodes are redeployed or restarted.
      # Replaces `form-new-cluster`, if `form-new-cluster` is set it takes precedence over this
      # property for backward compatibility
      new-cluster-enabled = on

      contact-point-discovery {

        service-name = safe-chat

        discovery-method = config

        port-name = akka.management.http.port #management

        # Interval at which service discovery will be polled in search for new contact-points
        interval = 1 second

        # The smallest number of contact points that need to be discovered before the bootstrap process can start.
        # For optimal safety during cluster formation, you may want to set these value to the number of initial
        # nodes that you know will participate in the cluster (e.g. the value of `spec.replicas` as set in your kubernetes config.
        required-contact-point-nr = 2

        # Amount of time for which a discovery observation must remain "stable"
        # (i.e. not change list of discovered contact-points) before a join decision can be made.
        # This is done to decrease the likelyhood of performing decisions on fluctuating observations.
        #
        # This timeout represents a tradeoff between safety and quickness of forming a new cluster.
        stable-margin = 3 seconds #5 seconds

        # Timeout for getting a reply from the service-discovery subsystem
        resolve-timeout = 3 seconds

      }
    }

    health-checks {
      readiness-path = "health/ready"
      liveness-path = "health/alive"
    }
  }

  #https://doc.akka.io/docs/akka/current/persistence-plugins.html#eager-initialization-of-persistence-plugin
  #extensions = [akka.persistence.Persistence]

  persistence {

    # When starting many persistent actors at the same time the journal
    # and its data store is protected from being overloaded by limiting number
    # of recoveries that can be in progress at the same time. When
    # exceeding the limit the actors will wait until other recoveries have
    # been completed.
    max-concurrent-recoveries = 32 #50


    //    journal {
    //      plugin = "akka.persistence.journal.leveldb"
    //      auto-start-journals = ["akka.persistence.journal.leveldb"]
    //    }
    //
    //    snapshot-store {
    //      plugin = "akka.persistence.snapshot-store.local"
    //      auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
    //    }


    #https://doc.akka.io/docs/akka/current/typed/persistence.html#replay-filter
    #https://blog.softwaremill.com/akka-cluster-split-brain-failures-are-you-ready-for-it-d9406b97e099
    #So if a split brain produces more wrong events than window-size then your aggregate state will be corrupted,
    # otherwise ....
    journal-plugin-fallback.replay-filter {
      # What the filter should do when detecting invalid events.
      # Supported values:
      # `repair-by-discard-old` : discard events from old writers,
      #                           warning is logged
      # `fail` : fail the replay, error is logged
      # `warn` : log warning but emit events untouched
      # `off`  : disable this feature completely
      mode = repair-by-discard-old

      window-size = 100
    }

    journal {
      plugin = "akka.persistence.cassandra.journal"
      auto-start-journals = [akka.persistence.cassandra.journal]
    }

    snapshot-store {
      plugin = "akka.persistence.cassandra.snapshot"
      auto-start-snapshot-stores = [akka.persistence.cassandra.snapshot]
    }

    cassandra {

      journal {

        plugin-dispatcher = cassandra-dispatcher #akka.actor.internal-dispatcher

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "chat"
        table = "chat_journal"

        # Maximum number of messages that will be batched when using `persistAsync`.
        # Also used as the max batch size for deletes.
        max-message-batch-size = 200

        # Target number of entries per partition (= columns per row).
        # Must not be changed after table creation (currently not checked).
        # This is "target" as AtomicWrites that span partition boundaries will result in bigger partitions to ensure atomicity.
        target-partition-size = 500000

        replication-factor = 1 #3

        support-all-persistence-ids = off
      }

      query {
        plugin-dispatcher = query-dispatcher

        # New events are retrieved (polled) with this interval.
        refresh-interval = 50 millis #3s

        # How many events to fetch in one query (replay) and keep buffered until they
        # are delivered downstreams.
        max-buffer-size = 500
      }

      events-by-tag {
        # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
        # false to avoid the overhead of maintaining the tag_views table.
        enabled = false
      }

      snapshot {

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "chat_snapshot"
        table = "chat_snapshots_journal"

        replication-factor = 1 #3
      }
    }
  }

  stream.materializer {
    initial-input-buffer-size = 16
    max-input-buffer-size = 16
    max-fixed-buffer-size = 16
    dispatcher = http-dispatcher

    stream-ref {
      buffer-capacity = 16
      subscription-timeout = 3 seconds
    }
  }
}

cassandra-dispatcher {
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 2
    parallelism-max = 8
  }
}

query-dispatcher {
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 2
    parallelism-max = 4
  }
}

http-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 2
    parallelism-max = 8
  }
}

fixed-thread-pool {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 5
  }
  throughput = 2
}

resizable-thread-pool {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    core-pool-size-min = 4
    core-pool-size-factor = 2.0
    core-pool-size-max = 8
  }
  throughput = 1000
}

datastax-java-driver {

  advanced {

    reconnect-on-init = true

    auth-provider {
      class = PlainTextAuthProvider
      username = ...
      password = ...
    }
  }

  basic {
    #contact-points = [""]
    load-balancing-policy.local-datacenter = "datacenter1"
  }

  profiles {
    akka-persistence-cassandra-profile {
      basic.request {
        #only for development
        consistency = ONE #QUORUM
      }
    }

    akka-persistence-cassandra-snapshot-profile {
      basic.request {
        #only for development
        consistency = ONE #QUORUM
      }
    }
  }
}


#see akka.coordination.lease.kubernetes for example
akka.coordination.lease.cassandra {

  lease-class = "akka.coordination.lease.cassandra.CassandraLease"

  # How often to write time into CRD so that if the holder crashes
  # another node can take the lease after a given timeout. If left blank then the default is
  # max(5s, heartbeat-timeout / 10) which will be 12s with the default heartbeat-timeout
  heartbeat-interval = 12s

  # How long a lease must not be updated before another node can assume
  # the holder has crashed.
  # If the lease holder hasn't crashed its next heart beat will fail due to the version
  # having been updated
  heartbeat-timeout = 120s


  # The amount of time to wait for a lease to be aquired or released. This includes all requests to the API
  # server that are required. If this timeout is hit then the lease *may* be taken due to the response being lost
  # on the way back from the API server but will be reported as not taken and can be safely retried.
  lease-operation-timeout = 8s # 5

  #stable-after = 7s + 1

}


safe-chat {
  passivation-after = 60 s #30 minutes
  snapshot-every = 50
  recent-history-size = 8
}